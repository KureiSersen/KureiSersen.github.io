<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>对数几率回归 on KureiSersen site</title>
    <link>https://kureisersen.github.io/tags/%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92/</link>
    <description>Recent content in 对数几率回归 on KureiSersen site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Feb 2024 22:11:30 +0800</lastBuildDate>
    <atom:link href="https://kureisersen.github.io/tags/%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>对数几率回归</title>
      <link>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Mon, 12 Feb 2024 22:11:30 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92/</guid>
      <description>特点 模型表达式： $$ \frac{1}{1+e^{-x}}\ 就是我们常说的sigmoid函数、激活函数、s型函数 $$&#xA;补充1： $$ 信息论中定义了一个概念，叫自信息。\ 自信息的期望被称为信息熵，信息熵用来衡量变量的不确定性，变量越不确定，信息熵越大\ 自信息：I(x)=-log_b\enspace p(x)\ b=2时自信息的单位为bit，b=e时自信息的单位为nat\ 信息熵：E(I(x))=-\sum\limits_{x}^{}p(x)log_b\enspace p(x) $$&#xA;补充2: $$ 相对熵，又称KL散度，可以用于衡量两个分布的差异\ 假设真是模型为p(x)，而我们求解得到的模型是q(x)，\ 那么我们就可以用p(x)与q（x）的相对熵作为LOSS函数\ D_{KL}(p||q) =-\sum\limits_{x}^{}p(x)log_b\enspace p(x)-\sum\limits_{x}^{}p(x)log_b\enspace q(x)\ 其中p(x)为常数,我们仅需使-\sum\limits_{x}^{}p(x)log_b\enspace q(x)部分最小,即可获得最优模型 $$&#xA;适用范围 求解方法 </description>
    </item>
  </channel>
</rss>
