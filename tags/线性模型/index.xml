<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>线性模型 on KureiSersen site</title>
    <link>https://kureisersen.github.io/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</link>
    <description>Recent content in 线性模型 on KureiSersen site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Feb 2024 22:11:30 +0800</lastBuildDate>
    <atom:link href="https://kureisersen.github.io/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>对数几率回归</title>
      <link>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Mon, 12 Feb 2024 22:11:30 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92/</guid>
      <description>特点 模型表达式： $$ \frac{1}{1+e^{-x}}\ 就是我们常说的sigmoid函数、激活函数、s型函数 $$&#xA;补充1： $$ 信息论中定义了一个概念，叫自信息。\ 自信息的期望被称为信息熵，信息熵用来衡量变量的不确定性，变量越不确定，信息熵越大\ 自信息：I(x)=-log_b\enspace p(x)\ b=2时自信息的单位为bit，b=e时自信息的单位为nat\ 信息熵：E(I(x))=-\sum\limits_{x}^{}p(x)log_b\enspace p(x) $$&#xA;补充2: $$ 相对熵，又称KL散度，可以用于衡量两个分布的差异\ 假设真是模型为p(x)，而我们求解得到的模型是q(x)，\ 那么我们就可以用p(x)与q（x）的相对熵作为LOSS函数\ D_{KL}(p||q) =-\sum\limits_{x}^{}p(x)log_b\enspace p(x)-\sum\limits_{x}^{}p(x)log_b\enspace q(x)\ 其中p(x)为常数,我们仅需使-\sum\limits_{x}^{}p(x)log_b\enspace q(x)部分最小,即可获得最优模型 $$&#xA;适用范围 求解方法 </description>
    </item>
    <item>
      <title>多元线性回归</title>
      <link>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Mon, 12 Feb 2024 22:10:37 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid>
      <description>特点 模型表达式： $$ f（x_i）=(w_1\enspace w_2 \enspace \cdots \enspace w_i)\begin{pmatrix} x_1 \ x_2 \\vdots\ x_i \end{pmatrix} + b\ 当然这个式子也可以通过化简b，从而写为 $$&#xA;$$ f（x_i）=(w_1\enspace w_2 \enspace \cdots \enspace w_i\enspace w_b)\begin{pmatrix} x_1 \ x_2 \\vdots\ x_i \ 1 \end{pmatrix} $$&#xA;适用范围 求解方法 最小二乘法估计 </description>
    </item>
    <item>
      <title>二分类线性判别分析</title>
      <link>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E4%BA%8C%E5%88%86%E7%B1%BB%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</link>
      <pubDate>Mon, 12 Feb 2024 22:09:36 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E4%BA%8C%E5%88%86%E7%B1%BB%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</guid>
      <description>特点 算法原理:&#xA;从几何的角度,找到一条穿过中心原点的投影线,让全体训练样本经过投影后:&#xA;异类样本的中心尽可能远 同类样本的方差尽可能小 模型推导: $$ 补充:2-范式:||x||2=(\sum\limits{i=1}^{N}|x_i|^2)^{\frac{1}{2}}就是在求向量的模长\ 设u_0、u_1分别是所有正样本、负样本在投影线上的投影中心\ 经过投影后,异类样本的中心尽可能远:max||w^Tu_0-w^Tu_1||{2}^{2}\ 经过投影后,同类样本的方差尽可能小:min\enspace w^T\sum\nolimits{0}w\ 那么联合两个式子,我们最终的损失函数就是:\ max\enspace J = \frac{||w^Tu_0-w^Tu_1||{2}^{2}}{w^T\sum\nolimits{0}w + w^T\sum\nolimits_{1}w} $$&#xA;适用范围 求解方法 拉格朗日乘数法 </description>
    </item>
    <item>
      <title>一元线性回归</title>
      <link>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Mon, 12 Feb 2024 22:07:34 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid>
      <description>特点 模型表达式： $$ f(x) = wx + b $$ 适用范围 求解数据具有==连续==的特征，比如身高从低到高，此时应使用 $$ f(x) = w_1 x_1 + b $$&#xA;在（2）式的基础上加入二值离散特征【颜值】（美：1，丑：0） $$ f(x)=w_1x_1+w_2x_2+b\ 此时w_1x_1项表示身高，w_2x_2表示颜值 $$&#xA;在（3）式的基础上加入有序的多值离散特征【饭量】（小：1，中：2，大：3） $$ f(x)=w_1x_1 +w_2x_2+w_3x_3+b\ 此时w_1x_1项表示身高，w_2x_2表示颜值，w_3x_3表示饭量 $$&#xA;在（4）式的基础上加入无序的多值离散特征【肤色】（黄：[1,0,0]，黑：[0,1,0]，白：[0,0,1]） $$ f(x)=w_1x_1 +w_2x_2+w_3x_3+w_4x_4+w_5x_5+w_6x_6+b\ 此时w_1x_1项表示身高，w_2x_2表示颜值，w_3x_3表示饭量\ 当肤色为黄色是，x_4，x_5，x_6代入值[1,0,0] $$&#xA;求解方法 最小二乘法估计 极大似然估计 </description>
    </item>
  </channel>
</rss>
