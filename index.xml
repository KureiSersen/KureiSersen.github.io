<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>KureiSersen site</title>
    <link>https://kureisersen.github.io/</link>
    <description>Recent content on KureiSersen site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Feb 2024 22:11:30 +0800</lastBuildDate>
    <atom:link href="https://kureisersen.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>对数几率回归</title>
      <link>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Mon, 12 Feb 2024 22:11:30 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92/</guid>
      <description>特点 模型表达式： $$ \frac{1}{1+e^{-x}}\ 就是我们常说的sigmoid函数、激活函数、s型函数 $$&#xA;补充1： $$ 信息论中定义了一个概念，叫自信息。\ 自信息的期望被称为信息熵，信息熵用来衡量变量的不确定性，变量越不确定，信息熵越大\ 自信息：I(x)=-log_b\enspace p(x)\ b=2时自信息的单位为bit，b=e时自信息的单位为nat\ 信息熵：E(I(x))=-\sum\limits_{x}^{}p(x)log_b\enspace p(x) $$&#xA;补充2: $$ 相对熵，又称KL散度，可以用于衡量两个分布的差异\ 假设真是模型为p(x)，而我们求解得到的模型是q(x)，\ 那么我们就可以用p(x)与q（x）的相对熵作为LOSS函数\ D_{KL}(p||q) =-\sum\limits_{x}^{}p(x)log_b\enspace p(x)-\sum\limits_{x}^{}p(x)log_b\enspace q(x)\ 其中p(x)为常数,我们仅需使-\sum\limits_{x}^{}p(x)log_b\enspace q(x)部分最小,即可获得最优模型 $$&#xA;适用范围 求解方法 </description>
    </item>
    <item>
      <title>多元线性回归</title>
      <link>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Mon, 12 Feb 2024 22:10:37 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid>
      <description>特点 模型表达式： $$ f（x_i）=(w_1\enspace w_2 \enspace \cdots \enspace w_i)\begin{pmatrix} x_1 \ x_2 \\vdots\ x_i \end{pmatrix} + b\ 当然这个式子也可以通过化简b，从而写为 $$&#xA;$$ f（x_i）=(w_1\enspace w_2 \enspace \cdots \enspace w_i\enspace w_b)\begin{pmatrix} x_1 \ x_2 \\vdots\ x_i \ 1 \end{pmatrix} $$&#xA;适用范围 求解方法 最小二乘法估计 </description>
    </item>
    <item>
      <title>二分类线性判别分析</title>
      <link>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E4%BA%8C%E5%88%86%E7%B1%BB%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</link>
      <pubDate>Mon, 12 Feb 2024 22:09:36 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E4%BA%8C%E5%88%86%E7%B1%BB%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</guid>
      <description>特点 算法原理:&#xA;从几何的角度,找到一条穿过中心原点的投影线,让全体训练样本经过投影后:&#xA;异类样本的中心尽可能远 同类样本的方差尽可能小 模型推导: $$ 补充:2-范式:||x||2=(\sum\limits{i=1}^{N}|x_i|^2)^{\frac{1}{2}}就是在求向量的模长\ 设u_0、u_1分别是所有正样本、负样本在投影线上的投影中心\ 经过投影后,异类样本的中心尽可能远:max||w^Tu_0-w^Tu_1||{2}^{2}\ 经过投影后,同类样本的方差尽可能小:min\enspace w^T\sum\nolimits{0}w\ 那么联合两个式子,我们最终的损失函数就是:\ max\enspace J = \frac{||w^Tu_0-w^Tu_1||{2}^{2}}{w^T\sum\nolimits{0}w + w^T\sum\nolimits_{1}w} $$&#xA;适用范围 求解方法 拉格朗日乘数法 </description>
    </item>
    <item>
      <title>一元线性回归</title>
      <link>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
      <pubDate>Mon, 12 Feb 2024 22:07:34 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</guid>
      <description>特点 模型表达式： $$ f(x) = wx + b $$ 适用范围 求解数据具有==连续==的特征，比如身高从低到高，此时应使用 $$ f(x) = w_1 x_1 + b $$&#xA;在（2）式的基础上加入二值离散特征【颜值】（美：1，丑：0） $$ f(x)=w_1x_1+w_2x_2+b\ 此时w_1x_1项表示身高，w_2x_2表示颜值 $$&#xA;在（3）式的基础上加入有序的多值离散特征【饭量】（小：1，中：2，大：3） $$ f(x)=w_1x_1 +w_2x_2+w_3x_3+b\ 此时w_1x_1项表示身高，w_2x_2表示颜值，w_3x_3表示饭量 $$&#xA;在（4）式的基础上加入无序的多值离散特征【肤色】（黄：[1,0,0]，黑：[0,1,0]，白：[0,0,1]） $$ f(x)=w_1x_1 +w_2x_2+w_3x_3+w_4x_4+w_5x_5+w_6x_6+b\ 此时w_1x_1项表示身高，w_2x_2表示颜值，w_3x_3表示饭量\ 当肤色为黄色是，x_4，x_5，x_6代入值[1,0,0] $$&#xA;求解方法 最小二乘法估计 极大似然估计 </description>
    </item>
    <item>
      <title>神经网络</title>
      <link>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Mon, 12 Feb 2024 22:06:08 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
      <description>特点 M-P神经元&#xA;定义：接受n个输入，这些输入通常是来自其他神经元，并给哥个输入赋予权重计算加权和，然后和自身特有的阈值，做减法进行比较，最后经过激活函数，模拟抑制和激活的过程，处理之后输出&#xA;模型表达式： $$ y=f(\sum\limits_{i=1}^{n}w_ix_i-\theta)=f(w^Tx+b) $$&#xA;分类：&#xA;单个M-P神经元：使用sgn、sigmoid作为激活函数，即为感知机 多个M-P神经元：神经网络 感知机&#xA;模型表达式：&#xA;sgn阶跃函数型感知机 $$ y=sgn(w^Tx-\theta)=\left{ \begin{aligned} 1 &amp;amp;,&amp;amp; w^Tx-\theta \geq0 \ 0 &amp;amp;,&amp;amp; w^Tx-\theta &amp;lt;0\ \end{aligned} \right. $$&#xA;sigmoid对数几率函数型感知机 $$ y=sigmoid(w^Tx-\theta)=\left{ \begin{aligned} 1 &amp;amp;,&amp;amp; w^Tx-\theta \geq0 \ 0 &amp;amp;,&amp;amp; w^Tx-\theta &amp;lt;0\ \end{aligned} \right. $$ 几何角度解释：&#xA;给定一个线性可分的数据集T，感知机的学习目标是求得能对数据集T中的福样本完全正确划分的超平面 $$ 其中w^T-\theta为超平面方程\ 1.超平面方程不唯一\ 2.法向量w垂直于超平面\ 3.法向量w和位移项\theta确定一个唯一的超平面\ 4.法向量w指向的那一半空间为证空间，另一半为负空间 $$ 求解策略：&#xA;将全体训练样本带入模型中找出误分类样本，此时误分类样本有且仅有两种可能&#xA;$$ w^Tx-\theta \geq0 ,模型输出为y&amp;rsquo;=1,样本正确输出应该为y=0 $$&#xA;$$ w^Tx-\theta \leq0 ,模型输出为y&amp;rsquo;=0,样本正确输出应该为y=1 $$&#xA;因此可以得到恒等式&#xA;$$ (y&amp;rsquo;-y)(w^Tx-\theta)\geq0 $$ 所以给定的数据集T，损失函数可以定义为</description>
    </item>
    <item>
      <title>决策树</title>
      <link>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E5%86%B3%E7%AD%96%E6%A0%91/</link>
      <pubDate>Mon, 12 Feb 2024 21:47:11 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E5%86%B3%E7%AD%96%E6%A0%91/</guid>
      <description>特点 算法原理&#xA;从逻辑角度,一堆if else语句的组合 从几何角度,根据某种准则划分特征空间 最终目的:将样本越分越纯 ID3决策树&#xA;补充:&#xA;详见对数几率回归中的自信息、信息熵&#xA;纯度与信息熵成反比，但似乎无法计算，只能依靠计算信息熵来反映&#xA;条件熵&#xA;$$ 假设现有关于变量a的集合D，计为a\in{a^1,a^2,a^3&amp;hellip;a^n}\ D^v表示当满足条件v时，变量a的集合，计为a\in{a^1,a^2,a^3&amp;hellip;a^n}\ \frac{|D^v|}{D}表示满足条件的a集合D^v，在所有a变量集合中的占比\ 那么满足条件v后，集合D的条件熵计为\sum\limits_{v=1}^{V}\frac{|D^v|}{D}Ent(D^v) $$&#xA;信息增益 $$ 在满足条件v后，变量a取值不确定的减少量，也即纯度的提升\ Gain(D,a)=Ent(D)-\sum\limits_{v=1}^{V}\frac{|D^v|}{D}Ent(D^v) $$&#xA;模型表达式：&#xA;$$ a_*=arg\enspace max\enspace Gain(D,a) $$ 模型缺陷：&#xA;信息增益准则对可能取值数目较多的属性有所偏好，造成每个取值里面所包含的样本量太少，极端情况下，每种特殊划分的取值中仅包含一个样本，在这种情况下，决策树模型过拟合失效 C4.5决策树&#xA;模型由来：&#xA;为解决ID3模型的缺陷，现引入权重IV来减少高取值数目属性的信息增益 因此引入新定义：增益率 $$ Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}\ 其中 IV(a)=-\sum\limits_{v=1}^{V}\frac{|D^v|}{D}log_2\frac{|D^v|}{D}，称为a的固有值\ a的取值个数V越大，通常IV(a)越大，\ 因此，增益率对可能取值数目较少的属性有所偏好，性质与信息增益正好相反 $$&#xA;改善后的模型思路：&#xA;先基于ID3模型，选择出信息增益率高于平均水平的属性，然后在基于增益率定义，从中选择出增益率最高的属性 用人话：先将所有属性按照增益率高低排序，选取前50%，保证基本盘；而后筛选掉其中取值数目较多的属性， CART决策树&#xA;有别于ID3的思路，引入一种新的概念基尼值用于衡量纯度 $$ 定义：从集合D中随机抽取两个样本，其类别标记不一致的概率。\ 因此，基尼值越小，碰到异类的概率越小，纯度越高。\ Gini(D)=\sum\limits_{k=1}^{|y|}\sum\limits_{k\neq1}^{}p_k p_k&amp;rsquo;\ =\sum\limits_{k=1}^{|y|}p_k(1-p_k)\ =1-\sum\limits_{k=1}^{|y|}p_k^2 $$&#xA;类比信息熵和条件熵，我们得出在条件v下的基尼指数 $$ Gini_index(D,a)=\sum\limits_{v=1}^{V}\frac{|D^v|}{D}Gini(D^v) $$&#xA;模型表达式： $$ a_*=arg\enspace max\enspace Gini_index(D,a) $$&#xA;实际构造算法： $$&#xA;基于每个属性a的每个可能取值v，将数据集D分为a=v和a\neq v两部分计算基尼值\ Gini_index(D,a)=\frac{|D^{a=v}|}{D}Gini(D^{a=v}) + \frac{|D^{a\neq v}|}{D}Gini(D^{a\neq v}) $$ $$ 2.</description>
    </item>
    <item>
      <title>导论</title>
      <link>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E5%AF%BC%E8%AE%BA/</link>
      <pubDate>Mon, 12 Feb 2024 21:43:13 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A5%BF%E7%93%9C%E4%B9%A6/%E5%AF%BC%E8%AE%BA/</guid>
      <description></description>
    </item>
    <item>
      <title>刑法</title>
      <link>https://kureisersen.github.io/post/%E6%B3%95%E5%AD%A6/%E5%88%91%E6%B3%95/</link>
      <pubDate>Mon, 12 Feb 2024 21:38:58 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%B3%95%E5%AD%A6/%E5%88%91%E6%B3%95/</guid>
      <description> 基础知识 什么是刑法 犯罪及其法律后果的总称，必须由这两部分组成（犯罪论、后果论） 1+11+1（一部刑法、11部修正案、一部单行刑法） 任何理论和解释都包含三种观念：正说、反说、折中说 行为无价值（行为本身错误）、结果无价值（造成了坏的结果） 行为无价值不能作为入罪的基础，不能因为行为本身错误就发动刑罚，但是行为无价值可以作为否定刑罚权的一种依据，如果一种行为是符合伦理道德的，那就不应该受到刑罚，即： 法益作为入罪的基础，伦理作为出罪的依据 刑法的机能 一方面惩罚犯罪、另一方面限制惩罚犯罪的权力本身（保证机能，保障人权） 如何来解释法律 法律一经制定就已经滞后了 形式解释和实质解释 形式解释：只关注语言的形式逻辑 实质解释：探究语言背后的精神 只有通过了形式解释的筛查，才能去探究背后的精神 主观解释和客观解释 主观解释：探究立法者的原意 客观解释：根据客观社会的实际需要 解释的分类： 按照解释的效益： 有权解释：有法律效益的解释 立法解释（全国人大常委会）、司法解释（最高人民法院、最高人民检察院），两者发生冲突时，立法解释优先 立法解释、司法解释不能类推 无权解释：无法律效益的解释 学理解释：学者所作，只能做参考 按照解释的方法： 文理解释：从文字符号上对语言进行解释 论理解释：如果文理解释的结论不唯一（不合理），那么启动论理解释 论理解释效果（限制或者扩张文字描述的范围）： 扩张效果 缩小效果 体系解释：要把法条放到整个刑法典中进行综合解释，甚至放到整个法律体系中进行综合解释，避免互相冲突，最终的效果即可能使扩张，又可能是缩小，但是仅有一种效果，不能即扩张同时又是缩小 当然解释：刑法条文没有明确规定，但实际上已经包含在法条的意思之中，从而可以在法条中自然而然推出的解释 入罪型：举轻以明重，必须要同时兼具形式解释和实质解释才可以量罪 出罪刑：举重以明轻，只要具备实质解释，哪怕是类推扩张了法条语义，也可以进行量刑 同类解释：对于法条中的等或者其他，应当按照所列举的内容、性质来进行解释，必须要有等价值性 反对解释：根据刑法条文的正面表述，来推导他的反面含义 补正解释：刑法中存在漏洞，通过修补的方法，对漏洞进行补正，重点在于正，而不在于补 目的解释： 刑法适用范围 </description>
    </item>
    <item>
      <title>法考科目大全</title>
      <link>https://kureisersen.github.io/post/%E6%B3%95%E5%AD%A6/%E6%B3%95%E8%80%83%E7%A7%91%E7%9B%AE%E5%A4%A7%E5%85%A8/</link>
      <pubDate>Mon, 12 Feb 2024 21:33:51 +0800</pubDate>
      <guid>https://kureisersen.github.io/post/%E6%B3%95%E5%AD%A6/%E6%B3%95%E8%80%83%E7%A7%91%E7%9B%AE%E5%A4%A7%E5%85%A8/</guid>
      <description></description>
    </item>
    <item>
      <title>Contact</title>
      <link>https://kureisersen.github.io/contact/</link>
      <pubDate>Mon, 12 Feb 2024 18:09:55 +0800</pubDate>
      <guid>https://kureisersen.github.io/contact/</guid>
      <description></description>
    </item>
    <item>
      <title>About</title>
      <link>https://kureisersen.github.io/about/</link>
      <pubDate>Mon, 12 Feb 2024 18:09:44 +0800</pubDate>
      <guid>https://kureisersen.github.io/about/</guid>
      <description></description>
    </item>
  </channel>
</rss>
